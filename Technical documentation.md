# 项目技术文档

## 1. 项目简介

本项目是一款创新的对话式互动游戏，融合了用户输入的故事背景、对话内容与实时采集的EEG脑电数据（情绪映射），通过AI技术动态实时生成剧情走向、图片背景和音乐氛围。项目采用前后端分离与微服务架构，支持多模态交互体验。

---

## 2. 系统架构

### 2.1 总体架构

```
┌────────────┐   用户输入/EEG   ┌────────────┐   AI生成   ┌────────────┐
│  前端界面   │───────────────→ │  后端服务   │─────────→  │ AI模型/服务 │
└────────────┘                 └────────────┘           └────────────┘
      ↑                             ↑
      └───────── WebSocket/HTTP ────┘
```

- **前端**：负责用户输入、对话展示、图片/音乐渲染、EEG状态监控。
- **后端**：负责EEG数据采集与情绪识别、剧情生成、图片生成、音乐生成服务的编排与调度。
- **AI服务**：包括文本生成（剧情）、图片生成、实时音乐生成等AI模型。

---

## 3. 主要模块与技术细节

### 3.1 前端（frontend/）

- **技术栈**：Vue 3、Vite、WebSocket、Fetch API
- **主要功能**：
  - 用户输入背景、对话
  - 实时展示AI生成的剧情、图片、音乐氛围
  - 监控EEG情绪状态（通过WebSocket与后端通信）
  - 音乐状态监控与错误处理
- **关键组件**：
  - `AudioStatusMonitor.vue`：实时音频状态与情绪监控
  - `AIChat.vue`：对话交互（如有，未见到代码，留空）
  - `TheWelcome.vue`、`Loading.vue`：界面辅助组件

#### 3.1.1 EEG情绪映射与前端联动

- 前端通过WebSocket监听后端推送的情绪状态（如`valence`、`arousal`、`emotion`标签），并据此动态调整音乐氛围、剧情走向和界面提示。
- **情绪状态与音乐/剧情/图片的具体映射规则**：

---

### 3.2 后端（EEG/）

- **技术栈**：Python 3.8+、FastAPI、Uvicorn、Pydantic、sounddevice、numpy、websocket-client、requests
- **主要服务**：
  - `brain_processor.py`：EEG数据采集与情绪识别
  - `audio_service.py`：音乐生成与播放
  - `cortex.py`：Emotiv Cortex SDK集成
  - `start_services.py`：服务启动与管理

#### 3.2.1 EEG数据采集与情绪识别

- 通过Emotiv Cortex SDK采集EEG脑电波数据。
- 实时分析EEG信号，计算情绪指标（如Valence效价、Arousal唤醒度）。
- **情绪识别算法**
    本系统采用Valence（效价）和Arousal（唤醒度）两个维度对EEG脑波数据进行情绪识别。
    具体映射规则如下：

    | 角度区间（度） | 英文标签      | 中文标签 |
    |:-------------:|:-------------:|:--------:|
    | 0~30          | Happy         | 开心     |
    | 30~60         | Excited       | 激动     |
    | 60~90         | Surprised     | 惊讶     |
    | 90~112.5      | Fear          | 恐惧     |
    | 112.5~135     | Angry         | 愤怒     |
    | 135~157.5     | Contempt      | 轻蔑     |
    | 157.5~180     | Disgust       | 厌恶     |
    | 180~198       | Miserable     | 痛苦     |
    | 198~216       | Sad           | 悲伤     |
    | 216~234       | Depressed     | 沮丧     |
    | 234~252       | Bored         | 无聊     |
    | 252~270       | Tired         | 疲惫     |
    | 270~300       | Sleepy        | 困倦     |
    | 300~330       | Relaxed       | 放松     |
    | 330~360       | Pleased       | 满足     |
    | 其它/低强度    | Neutral       | 中性     |

- 先根据EEG原始指标加权计算Valence和Arousal（范围-1~1），再用atan2(arousal, valence)得到角度，按区间分配情绪标签。
- 情绪强度低于阈值时，统一归为“中性”。

- 识别结果通过HTTP API推送给音乐服务和剧情生成模块。

#### 3.2.2 音乐生成服务

- 接收情绪数据，调用Google Lyria等AI音乐生成模型，动态调整音乐风格与参数。
- 支持实时音频流播放，前端通过WebSocket/HTTP获取状态。
- **音乐风格与情绪的映射规则**

情绪标签会进一步映射为详细的音乐风格参数，用于AI音乐生成。
主要参数包括基调、乐器、节奏、动态、氛围、织体等。

    | 情绪标签      | 基础风格           | 乐器                   | 节奏           | 动态           | 氛围           | 织体           |
    |:-------------:|:------------------|:----------------------|:--------------|:--------------|:--------------|:--------------|
    | 开心 (Happy)  | 明亮大调，欢快旋律 | 钢琴、弦乐、吉他       | 120-140 BPM   | 渐强，活泼     | 欢欣鼓舞       | 丰富分层       |
    | 激动 (Excited)| 动感节奏，和弦推进 | 电吉他、鼓、合成器     | 140-160 BPM   | 高能，戏剧性   | 紧张激烈       | 密集有力       |
    | 悲伤 (Sad)    | 小调，忧郁旋律     | 钢琴、大提琴、小提琴   | 50-70 BPM     | 柔和，情感丰富 | 深沉忧伤       | 简约抒情       |
    | 愤怒 (Angry)  | 激烈和弦，强烈节奏 | 失真吉他、重鼓、铜管   | 150-180 BPM   | 大声，冲击力强 | 激烈对抗       | 厚重密集       |
    | 恐惧 (Fear)   | 黑暗小调，色彩和声 | 低音弦乐、铜管、打击   | 70-120 BPM    | 突变，紧张     | 阴森悬疑       | 渐进层次       |
    | 沮丧 (Depressed)| 低音持续，和声静止| 低音弦乐、稀疏钢琴     | 40-60 BPM     | 持续低沉       | 压抑沉重       | 厚重静止       |
    | 中性 (Neutral)| 简单和声，稳定节奏 | 合成器、环境音         | 80-100 BPM    | 平稳           | 平静中性       | 简约背景       |
    | 无聊 (Bored)  | 重复模式，单调节奏 | 鼓机、合成器           | 90-110 BPM    | 单调无变化     | 机械无趣       | 稀疏重复       |
    | ...           | ...              | ...                    | ...           | ...           | ...           | ...           |

- 详细参数可参考`audio_service.py`中的`COMPLEX_EMOTION_MAPPING`字典。
- 该映射表为音乐生成服务提供Prompt和参数，确保音乐氛围与用户情绪实时同步。


#### 3.2.3 剧情与图片生成服务

- 用户输入背景、对话，结合EEG情绪数据，调用AI大模型（如GPT、Stable Diffusion等）生成后续剧情和图片。
- 剧情生成API、图片生成API的接口与调用流程——**[此处需补充具体实现/接口说明]**

---

## 4. 数据流与交互流程

1. 用户在前端输入背景、对话，佩戴EEG设备。
2. 后端`brain_processor.py`实时采集EEG数据，识别情绪状态。
3. 情绪数据推送至`audio_service.py`，生成并播放对应音乐，同时推送至前端用于界面联动。
4. 用户输入+情绪数据一并传递给剧情/图片生成模块，AI生成新的剧情文本和图片背景，前端实时展示。
5. 前端持续通过WebSocket/HTTP获取最新状态，支持错误处理与服务重启。

---

## 5. API接口文档（部分）

### 5.1 音乐服务API

- `GET /health`：健康检查
- `POST /update_emotion`：接收情绪数据
- `GET /audio_status`：获取音频流状态
- `POST /restart_audio`：重启音频服务
- `WebSocket /ws/audio_status`：实时推送音频状态

### 5.2 剧情/图片生成API

- `POST /generate_story`：输入用户对话、情绪，返回AI生成剧情（**[接口需补充]**）
- `POST /generate_image`：输入场景描述、情绪，返回AI生成图片（**[接口需补充]**）

---

## 6. 配置与部署

### 6.1 环境依赖

- Python依赖见`EEG/requirements.txt`
- Node.js依赖见`frontend/package.json`

### 6.2 启动流程

1. 安装后端依赖：`pip install -r EEG/requirements.txt`
2. 安装前端依赖：`npm install`（在frontend目录下）
3. 启动后端服务：`python EEG/start_services.py`
4. 启动前端服务：`npm run dev`（在frontend目录下）

---

## 7. 关键技术点与待补充内容

- **EEG情绪与剧情/图片/音乐的映射表**：需补充详细规则或算法。
- **剧情生成与图片生成的AI服务接口**：需补充API设计与实现细节。

---

## 8. 参考与扩展

- Emotiv Cortex SDK官方文档
- Google Lyria/GenAI官方文档
- Vue 3官方文档
- FastAPI官方文档

---

**注：本技术文档基于现有代码与项目结构撰写，部分功能与接口如未在代码中体现，已留空并标注，后续可补充完善。**

---

如需进一步细化某一模块或补充具体映射/算法，请告知！